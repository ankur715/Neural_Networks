{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  \n",
    "# from torch import optim  #optimizer\n",
    "import torch.nn.functional as F  #relu, softmax, etc.\n",
    "# import csv\n",
    "# import random\n",
    "# import re\n",
    "# import os\n",
    "# import unicodedata\n",
    "# import codecs\n",
    "# import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layer=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        # initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        # because our input sie is a word embedding with number of features == hidden_size\n",
    "        seld.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "    \n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # input_seq: batch of input sentences; shape=(max_length, batch_size)\n",
    "        # input_lengths: list of sentence lengths corresponding to each sentence in the batch\n",
    "        # hidden state, of shape: (n_layers x num_directions, batch_size, hidden_size)\n",
    "        # convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # pack padded batch of sequences for the RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # unpack padding\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputts[:,:,self.hidden_size:]\n",
    "        # return output and final hidden state\n",
    "        return outputs, hidden\n",
    "        # outputs: the output features h_t from the last layer of the GRU, for each timestep (sum of bidirectional outputs)\n",
    "        # outputs shape=(max_length, batch_size, hidden_size)\n",
    "        # hidden: hidden state for the last timestep, of shape=(n_layers x num_directions, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5500, -0.8938],\n",
       "         [-0.1461,  0.6787],\n",
       "         [ 0.2025,  0.8576],\n",
       "         [ 0.1137, -1.6217]],\n",
       "\n",
       "        [[-0.9726, -0.5376],\n",
       "         [-0.0859,  0.2042],\n",
       "         [ 1.0472, -1.5976],\n",
       "         [ 0.4279,  0.1457]],\n",
       "\n",
       "        [[-1.0674, -2.4240],\n",
       "         [-0.3903,  0.5151],\n",
       "         [ 0.3786,  0.7431],\n",
       "         [-0.1456,  0.0630]],\n",
       "\n",
       "        [[ 0.5778,  0.3427],\n",
       "         [-0.8794, -0.5316],\n",
       "         [-0.2524, -0.3511],\n",
       "         [-1.5978,  0.1027]],\n",
       "\n",
       "        [[-0.3802, -0.7170],\n",
       "         [-0.2591, -0.3310],\n",
       "         [-1.8272, -1.1497],\n",
       "         [-0.3459, -0.5028]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(5,4,3)\n",
    "a[:,:,:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Pack Padded Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(6,7)  #6 batches, max 7 words pre batch\n",
    "lengths = [7,7,6,5,4,2]  #length of each batch\n",
    "targets = pack_padded_sequence(a, lengths, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)\n",
    "print(targets[0])\n",
    "print(targets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing the Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# luong attention layer\n",
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        # element-wise multiply the current target decoder state with the encoder output and sum them\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden of shape: (1, batch_size, hidden_size)\n",
    "        # encoder_outputs of shape: (mmax_length, batch_size, hidden_size)\n",
    "        \n",
    "        # calculate the attention weights (energies)\n",
    "        attn_energies = self.dot_score(hidden, encoder_outputs)  #(max_length, batch_size)\n",
    "        # transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()  #(batch_size, max_length)\n",
    "        # return the softmax normalized probability score (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)  #(batch_size, 1, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the decoder, we will manually feed our batch one time step at a time. This means that our embedded ord tensor and GRU output will both have shape (1, batch_size, hidden_size). The steps are: Get embedding of current input word, Forward through unidirectional GRU, Calculate attention weights from the current GRU output, Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector, Concatenate weighted context vector and GRU output, Predict next word, and finally Return output and final hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8365, 0.5777, 0.8596, 0.7298, 0.7634, 0.4625, 0.6062],\n",
       "        [0.7194, 0.8295, 0.3138, 0.4103, 0.7714, 0.0983, 0.7104],\n",
       "        [0.5396, 0.9785, 0.5689, 0.8912, 0.4184, 0.8996, 0.2706],\n",
       "        [0.0667, 0.4212, 0.5936, 0.2655, 0.8596, 0.4887, 0.0099],\n",
       "        [0.9633, 0.7603, 0.5566, 0.5069, 0.6149, 0.8789, 0.5651]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(5,7)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1638, 0.1264, 0.1676, 0.1472, 0.1522, 0.1127, 0.1301],\n",
       "        [0.1640, 0.1830, 0.1093, 0.1204, 0.1727, 0.0881, 0.1625],\n",
       "        [0.1237, 0.1918, 0.1274, 0.1758, 0.1096, 0.1773, 0.0945],\n",
       "        [0.0998, 0.1423, 0.1691, 0.1218, 0.2206, 0.1522, 0.0943],\n",
       "        [0.1848, 0.1508, 0.1230, 0.1171, 0.1304, 0.1698, 0.1241]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = F.softmax(a, dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1638, 0.1264, 0.1676, 0.1472, 0.1522, 0.1127, 0.1301])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(b[0])\n",
    "b[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self,attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "        \n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # input_step: one time step (one word) of inout sequence batch; shape=(1,batch_size) \n",
    "        # last_hidden: final hidden layer of GRU; shape=(n_layers x num_directions, batch_size, hidden_size)\n",
    "        # encoder_outputs: encoder model's output; shape=(max_lengths, batch_size, hidden_size)\n",
    "        # note: we run this one step (batch of word) at a time\n",
    "        \n",
    "        # get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = step.embedding_dropout(embedded)\n",
    "        # forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # multiply attention weights to encoder outputs to get new weighted sum context vector\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
